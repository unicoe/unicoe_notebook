{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＃实验记录，之后就不用表格了很麻烦，在这里同步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SSD\n",
    "\n",
    "ssd300×300 focal loss \n",
    "batch_size：16\n",
    "\n",
    "train_net: \"models/VGGNet/VOC0712/SSD_300x300/train.prototxt\"\n",
    "test_net: \"models/VGGNet/VOC0712/SSD_300x300/test.prototxt\"\n",
    "test_iter: 503\n",
    "test_interval: 10000\n",
    "base_lr: 0.0002\n",
    "display: 20\n",
    "max_iter: 30000\n",
    "lr_policy: \"multistep\"\n",
    "gamma: 0.1\n",
    "momentum: 0.9\n",
    "weight_decay: 0.0005\n",
    "snapshot: 20000\n",
    "snapshot_prefix: \"models/VGGNet/VOC0712/SSD_300x300/VGG_VOC0712_SSD_300x300\"\n",
    "solver_mode: GPU\n",
    "device_id: 0\n",
    "debug_info: false\n",
    "snapshot_after_train: true\n",
    "test_initialization: false\n",
    "average_loss: 10\n",
    "\n",
    "stepvalue: 20000\n",
    "stepvalue: 30000\n",
    "iter_size: 1\n",
    "type: \"SGD\"\n",
    "eval_type: \"detection\"\n",
    "ap_version: \"11point\"\n",
    "\n",
    "ssd focal loss训练出来效果很差　39%,是比不用提升了一些，但是效果不好(不知道是不是迭代次数不够的原因)\n",
    "\n",
    "通过观察，之前给分数较低的框，现在给出的分数很高。　0.3以上的很多，这有问题。训练到最后，loss依旧较高\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD\n",
    "\n",
    "数据未变，使用更长的迭代周期，和优化算法。\n",
    "\n",
    "train_net: \"models/VGGNet/VOC0712/SSD_300x300/train.prototxt\"\n",
    "test_net: \"models/VGGNet/VOC0712/SSD_300x300/test.prototxt\"\n",
    "test_iter: 503\n",
    "test_interval: 10000\n",
    "base_lr: 0.0002\n",
    "display: 10\n",
    "max_iter: 120000\n",
    "lr_policy: \"multistep\"\n",
    "gamma: 0.1\n",
    "momentum: 0.9\n",
    "weight_decay: 0.0005\n",
    "snapshot: 20000\n",
    "snapshot_prefix: \"models/VGGNet/VOC0712/SSD_300x300/VGG_VOC0712_SSD_300x300\"\n",
    "solver_mode: GPU\n",
    "device_id: 0\n",
    "debug_info: false\n",
    "snapshot_after_train: true\n",
    "test_initialization: false\n",
    "average_loss: 10\n",
    "stepvalue: 90000\n",
    "stepvalue: 110000\n",
    "iter_size: 1\n",
    "type: \"Adam\"\n",
    "eval_type: \"detection\"\n",
    "ap_version: \"11point\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "这就是简单的调参，意义不大，要看到问题未表露出来的东西。由表及里。\n",
    "预计到明天８点左右就ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rfcn\n",
    "train_net: \"/home/user/Disk1.8T/py-R-FCN/experiments/5_28_original/train_agnostic_ohem.prototxt\"\n",
    "base_lr: 0.0002\n",
    "lr_policy: \"step\"\n",
    "gamma: 0.1\n",
    "stepsize: 50000\n",
    "display: 20\n",
    "\n",
    "momentum: 0.9\n",
    "weight_decay: 0.0005\n",
    "#### We disable standard caffe solver snapshotting and implement our own snapshot\n",
    "#### function\n",
    "snapshot: 0\n",
    "#### We still use the snapshot prefix, though\n",
    "snapshot_prefix: \"resnet50_rfcn_ohem\"\n",
    "iter_size: 8\n",
    "#### debug_info: true\n",
    "\n",
    "\n",
    "case $DATASET in\n",
    "  pascal_voc)\n",
    "    TRAIN_IMDB=\"voc_0712_trainval\"\n",
    "    TEST_IMDB=\"voc_0712_test\"\n",
    "    PT_DIR=\"pascal_voc\"\n",
    "    ITERS=32000\n",
    "    \n",
    "    \n",
    "rfcn的实验，我换用了　vis>0.6的数据。\n",
    "\n",
    "就是为了让数据更加合理，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
